{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e95c55-9ef3-40b5-b47f-8fdf45935e54",
   "metadata": {},
   "source": [
    "## Parsing a .bib file to DataFrame and .xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed7855-a8a2-4797-bdde-a1df02e4d20e",
   "metadata": {},
   "source": [
    "Install dependencies if not already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6490e79-7d79-4c47-9845-591144431ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install xlsxwriter\n",
    "!pip install bibtexparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b230d5-83a4-47fe-b0d2-95a21f48eb56",
   "metadata": {},
   "source": [
    "Parse the .bib file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27805926-9fcd-4976-a0ee-475c96d18c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>author</th>\n",
       "      <th>publisher</th>\n",
       "      <th>file</th>\n",
       "      <th>keywords</th>\n",
       "      <th>note</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OCR-free Document Understanding Transformer</th>\n",
       "      <td>October</td>\n",
       "      <td>2022</td>\n",
       "      <td>Kim, Geewook and Hong, Teakgyu and Yim, Moonbi...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Kim et al. - 2022 - OCR-free Document Understa...</td>\n",
       "      <td>Computer Science - Artificial Intelligence,Com...</td>\n",
       "      <td>Code: https://github.com/clovaai/donut</td>\n",
       "      <td>Understanding document images (e.g., invoices)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention Is All You Need</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>Vaswani, Ashish and Shazeer, Noam and Parmar, ...</td>\n",
       "      <td>Curran Associates, Inc.</td>\n",
       "      <td>Vaswani et al. - Attention is All you Need.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The famous transformer introduction paper</td>\n",
       "      <td>The dominant sequence transduction models are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unifying Vision, Text, and Layout for Universal Document Processing</th>\n",
       "      <td>December</td>\n",
       "      <td>2022</td>\n",
       "      <td>Tang, Zineng and Yang, Ziyi and Wang, Guoxin a...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Tang et al. - 2022 - Unifying Vision, Text, an...</td>\n",
       "      <td>Computer Science - Artificial Intelligence,Com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We propose Universal Document Processing (UDOP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Image-and-Language Understanding from Pixels Only</th>\n",
       "      <td>December</td>\n",
       "      <td>2022</td>\n",
       "      <td>Tschannen, Michael and Mustafa, Basil and Houl...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Tschannen et al. - 2022 - Image-and-Language U...</td>\n",
       "      <td>Computer Science - Computer Vision and Pattern...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multimodal models are becoming increasingly ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding</th>\n",
       "      <td>January</td>\n",
       "      <td>2022</td>\n",
       "      <td>Xu, Yang and Xu, Yiheng and Lv, Tengchao and C...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Xu et al. - 2022 - LayoutLMv2 Multi-modal Pre-...</td>\n",
       "      <td>Computer Science - Computation and Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pre-training of text and layout has proved eff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       month  year  \\\n",
       "title                                                                \n",
       "OCR-free Document Understanding Transformer          October  2022   \n",
       "Attention Is All You Need                                NaN  2017   \n",
       "Unifying Vision, Text, and Layout for Universal...  December  2022   \n",
       "Image-and-Language Understanding from Pixels Only   December  2022   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...   January  2022   \n",
       "\n",
       "                                                                                               author  \\\n",
       "title                                                                                                   \n",
       "OCR-free Document Understanding Transformer         Kim, Geewook and Hong, Teakgyu and Yim, Moonbi...   \n",
       "Attention Is All You Need                           Vaswani, Ashish and Shazeer, Noam and Parmar, ...   \n",
       "Unifying Vision, Text, and Layout for Universal...  Tang, Zineng and Yang, Ziyi and Wang, Guoxin a...   \n",
       "Image-and-Language Understanding from Pixels Only   Tschannen, Michael and Mustafa, Basil and Houl...   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...  Xu, Yang and Xu, Yiheng and Lv, Tengchao and C...   \n",
       "\n",
       "                                                                  publisher  \\\n",
       "title                                                                         \n",
       "OCR-free Document Understanding Transformer                           arXiv   \n",
       "Attention Is All You Need                           Curran Associates, Inc.   \n",
       "Unifying Vision, Text, and Layout for Universal...                    arXiv   \n",
       "Image-and-Language Understanding from Pixels Only                     arXiv   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...                    arXiv   \n",
       "\n",
       "                                                                                                 file  \\\n",
       "title                                                                                                   \n",
       "OCR-free Document Understanding Transformer         Kim et al. - 2022 - OCR-free Document Understa...   \n",
       "Attention Is All You Need                              Vaswani et al. - Attention is All you Need.pdf   \n",
       "Unifying Vision, Text, and Layout for Universal...  Tang et al. - 2022 - Unifying Vision, Text, an...   \n",
       "Image-and-Language Understanding from Pixels Only   Tschannen et al. - 2022 - Image-and-Language U...   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...  Xu et al. - 2022 - LayoutLMv2 Multi-modal Pre-...   \n",
       "\n",
       "                                                                                             keywords  \\\n",
       "title                                                                                                   \n",
       "OCR-free Document Understanding Transformer         Computer Science - Artificial Intelligence,Com...   \n",
       "Attention Is All You Need                                                                         NaN   \n",
       "Unifying Vision, Text, and Layout for Universal...  Computer Science - Artificial Intelligence,Com...   \n",
       "Image-and-Language Understanding from Pixels Only   Computer Science - Computer Vision and Pattern...   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...        Computer Science - Computation and Language   \n",
       "\n",
       "                                                                                         note  \\\n",
       "title                                                                                           \n",
       "OCR-free Document Understanding Transformer            Code: https://github.com/clovaai/donut   \n",
       "Attention Is All You Need                           The famous transformer introduction paper   \n",
       "Unifying Vision, Text, and Layout for Universal...                                        NaN   \n",
       "Image-and-Language Understanding from Pixels Only                                         NaN   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...                                        NaN   \n",
       "\n",
       "                                                                                             abstract  \n",
       "title                                                                                                  \n",
       "OCR-free Document Understanding Transformer         Understanding document images (e.g., invoices)...  \n",
       "Attention Is All You Need                           The dominant sequence transduction models are ...  \n",
       "Unifying Vision, Text, and Layout for Universal...  We propose Universal Document Processing (UDOP...  \n",
       "Image-and-Language Understanding from Pixels Only   Multimodal models are becoming increasingly ef...  \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...  Pre-training of text and layout has proved eff...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bib_parser import parse_bibtex\n",
    "from bib_formatter import save_xlsx\n",
    "\n",
    "# Specify bibtex fields you want to parse:\n",
    "fields = ['title', 'month', 'year', 'author', 'publisher', \n",
    "          'file', 'keywords', 'note', 'abstract']\n",
    "\n",
    "# Get the fields into DataFrame as columns (index=title):\n",
    "bib_df = parse_bibtex(fields, file='bibliography.bib')\n",
    "\n",
    "# Save to formatted .xlsx file:\n",
    "save_xlsx(bib_df, \"overview.xlsx\", cols=fields)\n",
    "\n",
    "# Display the DataFrame:\n",
    "bib_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e8565-252a-4789-9248-e6e5a32f014e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
