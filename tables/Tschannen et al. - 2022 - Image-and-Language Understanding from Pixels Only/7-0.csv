,[18] [19],"Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Sim- ple contrastive learning of sentence embeddings. In EMNLP, pages 6894-6910, 2021. 2 John M. Giorgi, Osvald Nitski, Bo Wang, and Gary D. Bader. DECLUTR: Deep contrastive learning for unsupervised tex-",[32],"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvu- nakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Andrew J Bal- lard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav"
0,[20],"tual representations. In ACL/IJCNLP, pages 879–895, 2021. 2 Rohit Girdhar, Alaaeldin El-Nouby, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra.",,"Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Pe- tersen, David Reiman, Ellen Clancy, Michal Zielinski, Mar- tin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, An-"
1,[21],"OmniMAE: Single model masked pretraining on images and videos. CoRR, abs/2206.08356, 2022. 1, 2 Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, and Ishan Misra. Omnivore: A single model for many visual modalities. In CVPR, pages",[33],"drew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure predic- tion with AlphaFold. Nature, 596(7873):583–589, 2021. 1 Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeong Yeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sang- doo Yun, Dongyoon Han, and Seunghyun Park. OCR-Free"
2,[22],"16081-16091, 2022. 1 Yuan Gong, Yu-An Chung, and James R. Glass. AST: au- dio spectrogram transformer. In Interspeech, pages 571-575, 2021. 1",[34],"document understanding transformer. In ECCV, pages 498– 517,2022. 2 Wonjae Kim, Bokyung Son, and Ildoo Kim. ViLT: Vision- and-language transformer without convolution or region su-"
3,[23] [24],"Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba- tra, and Devi Parikh. Making the V in VQA matter: El- evating the role of image understanding in visual question answering. In CVPR, 2017. 4, 13 Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B. Girshick. Masked autoencoders are scal-",[35],"and-language transformer without convolution or region su- pervision. In ICML, pages 5583–5594. PMLR, 2021. 5, 6, 17 Alexander Kolesnikov, André Susano Pinto, Lucas Beyer, Xiaohua Zhai, Jeremiah Harmsen, and Neil Houlsby. UViM: A unified modeling approach for vision with learned guiding codes. In NeurIPS, 2022. 1"
4,[25] [26] [27] [28],"able vision learners. In CVPR, pages 15979–15988, 2022. 2 Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey A. Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Sali- mans. Imagen video: High definition video generation with diffusion models. CoRR, abs/2210.02303, 2022. 2 Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, and Christoph Feichtenhofer. Masked autoencoders that listen. CORR, abs/2207.06405, 2022. 1 Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei Layoutlmv3: Pre-training for document AI with uni- fied text and image masking. In ACMMM, pages 4083–4091, 2022. 2 Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-BERT: Aligning image pixels with text by deep multi-modal transformers. CoRR, abs/2004.00849, 2020. 6",[36] [37] [38] [39] [40],"Taku Kudo and John Richardson. SentencePiece: A sim- ple and language independent subword tokenizer and detok- enizer for neural text processing. In EMNLP, 2018. 2,6 Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In ICML, pages 3744-3753, 2019. 3, 14 Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language under- standing. CoRR, abs/2210.03347, 2022. 2 Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A simple and performant baseline for vision and language. CoRR, abs/1908.03557, 2019. 6 Qing Li, Boqing Gong, Yin Cui, Dan Kondratyuk, Xianzhi Matthew Brown. Towards a uni-"
5,,,,"Du, Ming-Hsuan Yang, and Matthew"
6,[29],"Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Kop- pula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J. Hénaff, Matthew M. Botvinick, Andrew Zisser-",[41],"fied foundation model: Jointly pre-training transformers on unpaired images and text. CoRR, abs/2112.07074, 2021. 2 Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Ye- ung, and James Zou. Mind the gap: Understanding"
7,,"man, Oriol Vinyals, and João Carreira. Perceiver IO: A gen-",,the modality gap in multi-modal contrastive representation
8,[30] [31],"eral architecture for structured inputs & outputs. In ICLR, 2022. 1 Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. 2 Yu Jiang, Vivek Natarajan, Xinlei Chen, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. Pythia v0.1: The winning entry to the VQA challenge 2018. CoRR, abs/1807.09956, 2018. 5",[42] [43] [44],"learning. In NeurIPS, 2022. 2, 7, 8, 20, 21 Valerii Likhosherstov, Anurag Arnab, Krzysztof Choroman- ski, Mario Lucic, Yi Tay, Adrian Weller, and Mostafa De- hghani. Polyvit: Co-training vision transformers on images, videos and audio. CoRR, abs/2111.12993, 2021. 1, 2, 3 Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representations. In ICLR, 2018. 2,4 Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViL- BERT: Pretraining task-agnostic visiolinguistic representa-"
