,,"tions for vision-and-language tasks. In NeurIPS, pages 13– 23, 2019. 6",[57],Elizabeth vocabulary,
0,[45],"Elman Mansimov, Mitchell Stern, Mia Xu Chen, Orhan Firat, Jakob Uszkoreit, and Puneet Jain. Towards end-",[58],"EMNLP, pages Christoph",
1,,"to-end in-image neural machine translation. CORR,",[58],Christoph Robert,
2,[46],"abs/2010.10648, 2020. 1, 2,8 Sabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias Gallé, Arun Raja, Chenglei Si, Wilson Y. Lee, Benoît Sagot, and Samson Tan. Between words and characters: A brief history of open-vocabulary modeling and tokenization in NLP. CoRR, abs/2112.10508, 2021. 2",[59] [60],"Coombes, 400M: Open pairs. CoRR, Rico Sennrichmachine ACL, 2016. 2 Noam Shazeer",
3,[47] [48] [49] [50],"Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multimodal contrastive learn- ing with LIMOE: the language-image mixture of experts. In NeurIPS, 2022. 2, 3, 4 Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, and Kunio Kashino. Masked spectrogram model- ing using masked autoencoders for learning general-purpose audio representation. CoRR, abs/2204.12260, 2022. 1 Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, Mingxing Tan, and Quoc V. Le. Combined scaling for open-vocabulary image classification. CoRR, abs/2111.10050, 2021. 2 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen",[61] [62] [63] [64],"learning rates 3,14 Sheng Shen, Anna RohrbachKeutzer. How tasks? In ICLRBaohua Sun, Michael Lin. ing glyph Workshop on 140–151, 2019. Zineng Tang, TVLT: Textless 2022. 1 Ashish Thapliyalcut. Crossmodal",
4,,"Krueger, and Ilya Sutskever. Learning transferable visual",,modal Evaluation,
5,[51],"models from natural language supervision. In ICML, 2021. 1, 2, 3, 4, 7, 14 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67, 2020. 3, 4, 6, 19",[65] [66],"Ruben Villegasdermans, Saffar, Santiago Phenaki: textual descriptionAlex Wang, Omer Levy,",Hernan Variable
6,[52],"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,",,,
7,,"Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, pages 8821- 8831, 2021. 2",[67],benchmark and standing. In Jianfeng Wang,
8,[53] [54],"Nils Rethmeier and Isabelle Augenstein. A primer on con- trastive pretraining in language processing: Methods, lessons learned and perspectives. ACM Computing Surveys, 2021. 2 Phillip Rust, Jonas F. Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de Lhoneux, and Desmond Elliott. Lan- guage modelling with pixels. CoRR, abs/2207.06991, 2022. 1, 2, 3, 7, 18",[68],"Kevin Lin, Zhe GIT: A generative language. CoRRWenhui Wangiang Peng, hammed, Image as a",CoRR
9,[55] [56],"Phillip Rust, Jonas Pfeiffer, Ivan Vulic, Sebastian Ruder, and Iryna Gurevych. How good is your tokenizer? on the mono- lingual performance of multilingual language models. In ACL/IJCNLP, pages 3118-3135, 2021. 2 Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J.",[69] [70],"and vision-Zirui Wang, Tsvetkov, and model pretraining 8 Zhuofeng WuSun, and Hao representation",
10,,"Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In",[71],"Linting Xue, Sharan Narang",
