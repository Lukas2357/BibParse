,[6],"Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104-120. Springer, 2020. 9",[18],"Guillaume Jaume, Hazim Thiran. Funsd: A scanned documents. Document Analysis and volume 2, pages 1–6.",
0,[7],"Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In Interna- tional Conference on Machine Learning, pages 1931-1942. PMLR, 2021. 9",[19],"Marcin Kardas, Piotr Ruder, Sebastian RiedelAxcell: Automatic papers. arXiv preprint",
1,[8],"Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction- finetuned language models. arXiv preprint arXiv:2210.11416,",[20],"Geewook Kim, Teakgyu Nam, Jinyoung Park, doo Yun, Dongyoon document understanding",
2,[9],"2022. 7,9 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL, 2018. 5, 7, 15",[21],"ence on Computer 9 Wonjae Kim, Bokyung and-Language Transformer Supervision. In ICML,",
3,[10],"Łukasz Garncarek, Rafał Powalski, Tomasz Stanisławek, Bar- tosz Topolski, Piotr Halama, Michał Turski, and Filip Gral- iński. Lambert: layout-aware language modeling for infor- mation extraction. In International Conference on Document Analysis and Recognition, pages 532–547. Springer, 2021. 1, 7,9,15",[22] [23],"Diederik P Kingma and stochastic optimizationChen-Yu Lee, Chun-Guolong Su, Nan Huasuhisa Fujii, and Tomas beyond sequential",
4,[11] [12],"Jiuxiang Gu, Jason Kuen, Vlad I Morariu, Handong Zhao, Rajiv Jain, Nikolaos Barmpalios, Ani Nenkova, and Tong Sun. Unidoc: Unified pretraining framework for document understanding. Advances in Neural Information Processing Systems, 34:39–50, 2021. 1,9 Zhangxuan Gu, Changhua Meng, Ke Wang, Jun Lan,",[24],"extraction. arXiv preprint David Lewis, Gady David Grossman, and lection for complex Proceedings of the 29th conference on Research",
5,,"Weiqiang Wang, Ming Gu, and Liqing Zhang. Xylayoutlm: Towards layout-aware multimodal networks for visually-rich document understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4583-4592, 2022. 1,9",[25],"trieval, pages 665–666Chenliang Li, Bin Bi, Fei Huang, and Luo Sifor form understanding2021. 1, 7, 15",
6,[13] [14],"Adam W Harley, Alex Ufkes, and Konstantinos G Derpanis. Evaluation of deep convolutional nets for document image classification and retrieval. In 2015 13th International Confer- ence on Document Analysis and Recognition (ICDAR), pages 991–995. IEEE, 2015. 1, 2, 6, 12 Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr",[26] [27],"Liunian Harold Li, and Kai-Wei Chang. mant baseline for arXiv:1908.03557, 2019. Minghao Li, Yiheng Wei, Zhoujun Li, and",
7,,"Dollár, and Ross Girshick. Masked autoencoders are scalable",,mark dataset for document,
8,,"vision learners. arXiv preprint arXiv:2111.06377, 2021. 2, 4, 5,6",[28],"arXiv:2006.01038, 2020. Peizhao Li, Jiuxiang",
9,[15] [16] [17],"Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, and Sungrae Park. Bros: A pre-trained lan- guage model focusing on text and layout for better key in- formation extraction from documents. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10767-10775, 2022. 1, 7, 9, 15 Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for document ai with unified text and image masking. arXiv preprint arXiv:2204.08387, 2022. 1, 3, 6, 7, 9, 15 Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849, 2020. 1",[29] [30],"dong Zhao, Rajiv JainSelfdoc: Self-supervised In Proceedings of the Vision and Pattern Yulin Li, Yuxi Qian, Zhang, Yan Liu, Kun Ding. Structext: modal transformers. In national Conference on 1 Jiasen Lu, Christopher taghi, and Aniruddha for vision, language, arXiv:2206.08916, 2022.",","
