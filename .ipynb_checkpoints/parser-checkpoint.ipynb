{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e95c55-9ef3-40b5-b47f-8fdf45935e54",
   "metadata": {},
   "source": [
    "## Parsing a .bib file to DataFrame and .xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed7855-a8a2-4797-bdde-a1df02e4d20e",
   "metadata": {},
   "source": [
    "Install dependencies if not already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6490e79-7d79-4c47-9845-591144431ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install xlsxwriter\n",
    "!pip install bibtexparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b230d5-83a4-47fe-b0d2-95a21f48eb56",
   "metadata": {},
   "source": [
    "Parse the .bib file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27805926-9fcd-4976-a0ee-475c96d18c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>author</th>\n",
       "      <th>publisher</th>\n",
       "      <th>file</th>\n",
       "      <th>keywords</th>\n",
       "      <th>note</th>\n",
       "      <th>abstract</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DocFormer: End-to-End Transformer for Document Understanding</th>\n",
       "      <td>September</td>\n",
       "      <td>2021</td>\n",
       "      <td>Appalaraju, Srikar and Jasani, Bhavan and Kota...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Appalaraju et al. - 2021 - DocFormer End-to-En...</td>\n",
       "      <td>Computer Science - Computer Vision and Pattern...</td>\n",
       "      <td>Good image for comparison of VDU methods on pa...</td>\n",
       "      <td>We present DocFormer -- a multi-modal transfor...</td>\n",
       "      <td>VDU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</th>\n",
       "      <td>May</td>\n",
       "      <td>2019</td>\n",
       "      <td>Devlin, Jacob and Chang, Ming-Wei and Lee, Ken...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Devlin et al. - 2019 - BERT Pre-training of De...</td>\n",
       "      <td>Computer Science - Computation and Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "      <td>Transformer Language Model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking</th>\n",
       "      <td>July</td>\n",
       "      <td>2022</td>\n",
       "      <td>Huang, Yupan and Lv, Tengchao and Cui, Lei and...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Huang et al. - 2022 - LayoutLMv3 Pre-training ...</td>\n",
       "      <td>Computer Science - Computation and Language,Co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Self-supervised pre-training techniques have a...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OCR-free Document Understanding Transformer</th>\n",
       "      <td>October</td>\n",
       "      <td>2022</td>\n",
       "      <td>Kim, Geewook and Hong, Teakgyu and Yim, Moonbi...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Kim et al. - 2022 - OCR-free Document Understa...</td>\n",
       "      <td>Computer Science - Artificial Intelligence,Com...</td>\n",
       "      <td>Code: https://github.com/clovaai/donut</td>\n",
       "      <td>Understanding document images (e.g., invoices)...</td>\n",
       "      <td>VDU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</th>\n",
       "      <td>October</td>\n",
       "      <td>2019</td>\n",
       "      <td>Lewis, Mike and Liu, Yinhan and Goyal, Naman a...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Lewis et al. - 2019 - BART Denoising Sequence-...</td>\n",
       "      <td>Computer Science - Computation and Language,Co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We present BART, a denoising autoencoder for p...</td>\n",
       "      <td>Transformer Language Model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention Is All You Need</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>Vaswani, Ashish and Shazeer, Noam and Parmar, ...</td>\n",
       "      <td>Curran Associates, Inc.</td>\n",
       "      <td>Vaswani et al. - Attention is All you Need.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The famous transformer introduction paper</td>\n",
       "      <td>The dominant sequence transduction models are ...</td>\n",
       "      <td>Transformer Language Model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unifying Vision, Text, and Layout for Universal Document Processing</th>\n",
       "      <td>December</td>\n",
       "      <td>2022</td>\n",
       "      <td>Tang, Zineng and Yang, Ziyi and Wang, Guoxin a...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Tang et al. - 2022 - Unifying Vision, Text, an...</td>\n",
       "      <td>Computer Science - Artificial Intelligence,Com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We propose Universal Document Processing (UDOP...</td>\n",
       "      <td>VDU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Image-and-Language Understanding from Pixels Only</th>\n",
       "      <td>December</td>\n",
       "      <td>2022</td>\n",
       "      <td>Tschannen, Michael and Mustafa, Basil and Houl...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Tschannen et al. - 2022 - Image-and-Language U...</td>\n",
       "      <td>Computer Science - Computer Vision and Pattern...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multimodal models are becoming increasingly ef...</td>\n",
       "      <td>VDU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding</th>\n",
       "      <td>January</td>\n",
       "      <td>2022</td>\n",
       "      <td>Xu, Yang and Xu, Yiheng and Lv, Tengchao and C...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Xu et al. - 2022 - LayoutLMv2 Multi-modal Pre-...</td>\n",
       "      <td>Computer Science - Computation and Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pre-training of text and layout has proved eff...</td>\n",
       "      <td>VDU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        month  year  \\\n",
       "title                                                                 \n",
       "DocFormer: End-to-End Transformer for Document ...  September  2021   \n",
       "BERT: Pre-training of Deep Bidirectional Transf...        May  2019   \n",
       "LayoutLMv3: Pre-training for Document AI with U...       July  2022   \n",
       "OCR-free Document Understanding Transformer           October  2022   \n",
       "BART: Denoising Sequence-to-Sequence Pre-traini...    October  2019   \n",
       "Attention Is All You Need                                 NaN  2017   \n",
       "Unifying Vision, Text, and Layout for Universal...   December  2022   \n",
       "Image-and-Language Understanding from Pixels Only    December  2022   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...    January  2022   \n",
       "\n",
       "                                                                                               author  \\\n",
       "title                                                                                                   \n",
       "DocFormer: End-to-End Transformer for Document ...  Appalaraju, Srikar and Jasani, Bhavan and Kota...   \n",
       "BERT: Pre-training of Deep Bidirectional Transf...  Devlin, Jacob and Chang, Ming-Wei and Lee, Ken...   \n",
       "LayoutLMv3: Pre-training for Document AI with U...  Huang, Yupan and Lv, Tengchao and Cui, Lei and...   \n",
       "OCR-free Document Understanding Transformer         Kim, Geewook and Hong, Teakgyu and Yim, Moonbi...   \n",
       "BART: Denoising Sequence-to-Sequence Pre-traini...  Lewis, Mike and Liu, Yinhan and Goyal, Naman a...   \n",
       "Attention Is All You Need                           Vaswani, Ashish and Shazeer, Noam and Parmar, ...   \n",
       "Unifying Vision, Text, and Layout for Universal...  Tang, Zineng and Yang, Ziyi and Wang, Guoxin a...   \n",
       "Image-and-Language Understanding from Pixels Only   Tschannen, Michael and Mustafa, Basil and Houl...   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...  Xu, Yang and Xu, Yiheng and Lv, Tengchao and C...   \n",
       "\n",
       "                                                                  publisher  \\\n",
       "title                                                                         \n",
       "DocFormer: End-to-End Transformer for Document ...                    arXiv   \n",
       "BERT: Pre-training of Deep Bidirectional Transf...                    arXiv   \n",
       "LayoutLMv3: Pre-training for Document AI with U...                    arXiv   \n",
       "OCR-free Document Understanding Transformer                           arXiv   \n",
       "BART: Denoising Sequence-to-Sequence Pre-traini...                    arXiv   \n",
       "Attention Is All You Need                           Curran Associates, Inc.   \n",
       "Unifying Vision, Text, and Layout for Universal...                    arXiv   \n",
       "Image-and-Language Understanding from Pixels Only                     arXiv   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...                    arXiv   \n",
       "\n",
       "                                                                                                 file  \\\n",
       "title                                                                                                   \n",
       "DocFormer: End-to-End Transformer for Document ...  Appalaraju et al. - 2021 - DocFormer End-to-En...   \n",
       "BERT: Pre-training of Deep Bidirectional Transf...  Devlin et al. - 2019 - BERT Pre-training of De...   \n",
       "LayoutLMv3: Pre-training for Document AI with U...  Huang et al. - 2022 - LayoutLMv3 Pre-training ...   \n",
       "OCR-free Document Understanding Transformer         Kim et al. - 2022 - OCR-free Document Understa...   \n",
       "BART: Denoising Sequence-to-Sequence Pre-traini...  Lewis et al. - 2019 - BART Denoising Sequence-...   \n",
       "Attention Is All You Need                              Vaswani et al. - Attention is All you Need.pdf   \n",
       "Unifying Vision, Text, and Layout for Universal...  Tang et al. - 2022 - Unifying Vision, Text, an...   \n",
       "Image-and-Language Understanding from Pixels Only   Tschannen et al. - 2022 - Image-and-Language U...   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...  Xu et al. - 2022 - LayoutLMv2 Multi-modal Pre-...   \n",
       "\n",
       "                                                                                             keywords  \\\n",
       "title                                                                                                   \n",
       "DocFormer: End-to-End Transformer for Document ...  Computer Science - Computer Vision and Pattern...   \n",
       "BERT: Pre-training of Deep Bidirectional Transf...        Computer Science - Computation and Language   \n",
       "LayoutLMv3: Pre-training for Document AI with U...  Computer Science - Computation and Language,Co...   \n",
       "OCR-free Document Understanding Transformer         Computer Science - Artificial Intelligence,Com...   \n",
       "BART: Denoising Sequence-to-Sequence Pre-traini...  Computer Science - Computation and Language,Co...   \n",
       "Attention Is All You Need                                                                         NaN   \n",
       "Unifying Vision, Text, and Layout for Universal...  Computer Science - Artificial Intelligence,Com...   \n",
       "Image-and-Language Understanding from Pixels Only   Computer Science - Computer Vision and Pattern...   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...        Computer Science - Computation and Language   \n",
       "\n",
       "                                                                                                 note  \\\n",
       "title                                                                                                   \n",
       "DocFormer: End-to-End Transformer for Document ...  Good image for comparison of VDU methods on pa...   \n",
       "BERT: Pre-training of Deep Bidirectional Transf...                                                NaN   \n",
       "LayoutLMv3: Pre-training for Document AI with U...                                                NaN   \n",
       "OCR-free Document Understanding Transformer                    Code: https://github.com/clovaai/donut   \n",
       "BART: Denoising Sequence-to-Sequence Pre-traini...                                                NaN   \n",
       "Attention Is All You Need                                   The famous transformer introduction paper   \n",
       "Unifying Vision, Text, and Layout for Universal...                                                NaN   \n",
       "Image-and-Language Understanding from Pixels Only                                                 NaN   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...                                                NaN   \n",
       "\n",
       "                                                                                             abstract  \\\n",
       "title                                                                                                   \n",
       "DocFormer: End-to-End Transformer for Document ...  We present DocFormer -- a multi-modal transfor...   \n",
       "BERT: Pre-training of Deep Bidirectional Transf...  We introduce a new language representation mod...   \n",
       "LayoutLMv3: Pre-training for Document AI with U...  Self-supervised pre-training techniques have a...   \n",
       "OCR-free Document Understanding Transformer         Understanding document images (e.g., invoices)...   \n",
       "BART: Denoising Sequence-to-Sequence Pre-traini...  We present BART, a denoising autoencoder for p...   \n",
       "Attention Is All You Need                           The dominant sequence transduction models are ...   \n",
       "Unifying Vision, Text, and Layout for Universal...  We propose Universal Document Processing (UDOP...   \n",
       "Image-and-Language Understanding from Pixels Only   Multimodal models are becoming increasingly ef...   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...  Pre-training of text and layout has proved eff...   \n",
       "\n",
       "                                                                      category  \n",
       "title                                                                           \n",
       "DocFormer: End-to-End Transformer for Document ...                         VDU  \n",
       "BERT: Pre-training of Deep Bidirectional Transf...  Transformer Language Model  \n",
       "LayoutLMv3: Pre-training for Document AI with U...                         NaN  \n",
       "OCR-free Document Understanding Transformer                                VDU  \n",
       "BART: Denoising Sequence-to-Sequence Pre-traini...  Transformer Language Model  \n",
       "Attention Is All You Need                           Transformer Language Model  \n",
       "Unifying Vision, Text, and Layout for Universal...                         VDU  \n",
       "Image-and-Language Understanding from Pixels Only                          VDU  \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...                         VDU  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bib_parser import parse_bibtex\n",
    "from bib_formatter import save_xlsx\n",
    "\n",
    "# Specify bibtex fields you want to parse. You can also manually add columns in the xlsx\n",
    "# file ones it is created, that are kept. Add those here as well:\n",
    "fields = ['title', 'month', 'year', 'category', 'author', 'publisher', \n",
    "          'file', 'note', 'abstract']\n",
    "\n",
    "# Get the fields into DataFrame as columns (index=title):\n",
    "bib_df = parse_bibtex(fields, file='bibliography.bib')\n",
    "\n",
    "# Save to formatted .xlsx file:\n",
    "save_xlsx(bib_df, \"overview.xlsx\", cols=fields)\n",
    "\n",
    "# Display the DataFrame:\n",
    "bib_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e8565-252a-4789-9248-e6e5a32f014e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
