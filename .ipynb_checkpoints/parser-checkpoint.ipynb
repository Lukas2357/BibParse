{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e95c55-9ef3-40b5-b47f-8fdf45935e54",
   "metadata": {},
   "source": [
    "## Parsing a .bib file to DataFrame and .xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed7855-a8a2-4797-bdde-a1df02e4d20e",
   "metadata": {},
   "source": [
    "Install dependencies if not already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6490e79-7d79-4c47-9845-591144431ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/lukas/anaconda3/envs/JobSearch/lib/python3.10/site-packages (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/lukas/anaconda3/envs/JobSearch/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/lukas/anaconda3/envs/JobSearch/lib/python3.10/site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/lukas/anaconda3/envs/JobSearch/lib/python3.10/site-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/lukas/anaconda3/envs/JobSearch/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: xlsxwriter in /home/lukas/anaconda3/envs/JobSearch/lib/python3.10/site-packages (3.0.3)\n",
      "Collecting bibtexparser\n",
      "  Using cached bibtexparser-1.4.0.tar.gz (51 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.3 in /home/lukas/anaconda3/envs/JobSearch/lib/python3.10/site-packages (from bibtexparser) (3.0.9)\n",
      "Building wheels for collected packages: bibtexparser\n",
      "  Building wheel for bibtexparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bibtexparser: filename=bibtexparser-1.4.0-py3-none-any.whl size=42430 sha256=ea96d5cc2e765b51db7ef4a20e300d8d3d6b85abe83cff0a496c1584a0f9071b\n",
      "  Stored in directory: /home/lukas/.cache/pip/wheels/5d/de/d5/4e42fd75d48106e7c4023d6594c2992db38afa7019f96139a2\n",
      "Successfully built bibtexparser\n",
      "Installing collected packages: bibtexparser\n",
      "Successfully installed bibtexparser-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install xlsxwriter\n",
    "!pip install bibtexparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b230d5-83a4-47fe-b0d2-95a21f48eb56",
   "metadata": {},
   "source": [
    "Parse the .bib file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27805926-9fcd-4976-a0ee-475c96d18c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>author</th>\n",
       "      <th>publisher</th>\n",
       "      <th>file</th>\n",
       "      <th>keywords</th>\n",
       "      <th>note</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</th>\n",
       "      <td>May</td>\n",
       "      <td>2019</td>\n",
       "      <td>Devlin, Jacob and Chang, Ming-Wei and Lee, Ken...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Devlin et al. - 2019 - BERT Pre-training of De...</td>\n",
       "      <td>Computer Science - Computation and Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We introduce a new language representation mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OCR-free Document Understanding Transformer</th>\n",
       "      <td>October</td>\n",
       "      <td>2022</td>\n",
       "      <td>Kim, Geewook and Hong, Teakgyu and Yim, Moonbi...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Kim et al. - 2022 - OCR-free Document Understa...</td>\n",
       "      <td>Computer Science - Artificial Intelligence,Com...</td>\n",
       "      <td>Code: https://github.com/clovaai/donut</td>\n",
       "      <td>Understanding document images (e.g., invoices)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</th>\n",
       "      <td>October</td>\n",
       "      <td>2019</td>\n",
       "      <td>Lewis, Mike and Liu, Yinhan and Goyal, Naman a...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Lewis et al. - 2019 - BART Denoising Sequence-...</td>\n",
       "      <td>Computer Science - Computation and Language,Co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We present BART, a denoising autoencoder for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention Is All You Need</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2017</td>\n",
       "      <td>Vaswani, Ashish and Shazeer, Noam and Parmar, ...</td>\n",
       "      <td>Curran Associates, Inc.</td>\n",
       "      <td>Vaswani et al. - Attention is All you Need.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The famous transformer introduction paper</td>\n",
       "      <td>The dominant sequence transduction models are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unifying Vision, Text, and Layout for Universal Document Processing</th>\n",
       "      <td>December</td>\n",
       "      <td>2022</td>\n",
       "      <td>Tang, Zineng and Yang, Ziyi and Wang, Guoxin a...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Tang et al. - 2022 - Unifying Vision, Text, an...</td>\n",
       "      <td>Computer Science - Artificial Intelligence,Com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We propose Universal Document Processing (UDOP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Image-and-Language Understanding from Pixels Only</th>\n",
       "      <td>December</td>\n",
       "      <td>2022</td>\n",
       "      <td>Tschannen, Michael and Mustafa, Basil and Houl...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Tschannen et al. - 2022 - Image-and-Language U...</td>\n",
       "      <td>Computer Science - Computer Vision and Pattern...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multimodal models are becoming increasingly ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding</th>\n",
       "      <td>January</td>\n",
       "      <td>2022</td>\n",
       "      <td>Xu, Yang and Xu, Yiheng and Lv, Tengchao and C...</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>Xu et al. - 2022 - LayoutLMv2 Multi-modal Pre-...</td>\n",
       "      <td>Computer Science - Computation and Language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pre-training of text and layout has proved eff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       month  year  \\\n",
       "title                                                                \n",
       "BERT: Pre-training of Deep Bidirectional Transf...       May  2019   \n",
       "OCR-free Document Understanding Transformer          October  2022   \n",
       "BART: Denoising Sequence-to-Sequence Pre-traini...   October  2019   \n",
       "Attention Is All You Need                                NaN  2017   \n",
       "Unifying Vision, Text, and Layout for Universal...  December  2022   \n",
       "Image-and-Language Understanding from Pixels Only   December  2022   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...   January  2022   \n",
       "\n",
       "                                                                                               author  \\\n",
       "title                                                                                                   \n",
       "BERT: Pre-training of Deep Bidirectional Transf...  Devlin, Jacob and Chang, Ming-Wei and Lee, Ken...   \n",
       "OCR-free Document Understanding Transformer         Kim, Geewook and Hong, Teakgyu and Yim, Moonbi...   \n",
       "BART: Denoising Sequence-to-Sequence Pre-traini...  Lewis, Mike and Liu, Yinhan and Goyal, Naman a...   \n",
       "Attention Is All You Need                           Vaswani, Ashish and Shazeer, Noam and Parmar, ...   \n",
       "Unifying Vision, Text, and Layout for Universal...  Tang, Zineng and Yang, Ziyi and Wang, Guoxin a...   \n",
       "Image-and-Language Understanding from Pixels Only   Tschannen, Michael and Mustafa, Basil and Houl...   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...  Xu, Yang and Xu, Yiheng and Lv, Tengchao and C...   \n",
       "\n",
       "                                                                  publisher  \\\n",
       "title                                                                         \n",
       "BERT: Pre-training of Deep Bidirectional Transf...                    arXiv   \n",
       "OCR-free Document Understanding Transformer                           arXiv   \n",
       "BART: Denoising Sequence-to-Sequence Pre-traini...                    arXiv   \n",
       "Attention Is All You Need                           Curran Associates, Inc.   \n",
       "Unifying Vision, Text, and Layout for Universal...                    arXiv   \n",
       "Image-and-Language Understanding from Pixels Only                     arXiv   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...                    arXiv   \n",
       "\n",
       "                                                                                                 file  \\\n",
       "title                                                                                                   \n",
       "BERT: Pre-training of Deep Bidirectional Transf...  Devlin et al. - 2019 - BERT Pre-training of De...   \n",
       "OCR-free Document Understanding Transformer         Kim et al. - 2022 - OCR-free Document Understa...   \n",
       "BART: Denoising Sequence-to-Sequence Pre-traini...  Lewis et al. - 2019 - BART Denoising Sequence-...   \n",
       "Attention Is All You Need                              Vaswani et al. - Attention is All you Need.pdf   \n",
       "Unifying Vision, Text, and Layout for Universal...  Tang et al. - 2022 - Unifying Vision, Text, an...   \n",
       "Image-and-Language Understanding from Pixels Only   Tschannen et al. - 2022 - Image-and-Language U...   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...  Xu et al. - 2022 - LayoutLMv2 Multi-modal Pre-...   \n",
       "\n",
       "                                                                                             keywords  \\\n",
       "title                                                                                                   \n",
       "BERT: Pre-training of Deep Bidirectional Transf...        Computer Science - Computation and Language   \n",
       "OCR-free Document Understanding Transformer         Computer Science - Artificial Intelligence,Com...   \n",
       "BART: Denoising Sequence-to-Sequence Pre-traini...  Computer Science - Computation and Language,Co...   \n",
       "Attention Is All You Need                                                                         NaN   \n",
       "Unifying Vision, Text, and Layout for Universal...  Computer Science - Artificial Intelligence,Com...   \n",
       "Image-and-Language Understanding from Pixels Only   Computer Science - Computer Vision and Pattern...   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...        Computer Science - Computation and Language   \n",
       "\n",
       "                                                                                         note  \\\n",
       "title                                                                                           \n",
       "BERT: Pre-training of Deep Bidirectional Transf...                                        NaN   \n",
       "OCR-free Document Understanding Transformer            Code: https://github.com/clovaai/donut   \n",
       "BART: Denoising Sequence-to-Sequence Pre-traini...                                        NaN   \n",
       "Attention Is All You Need                           The famous transformer introduction paper   \n",
       "Unifying Vision, Text, and Layout for Universal...                                        NaN   \n",
       "Image-and-Language Understanding from Pixels Only                                         NaN   \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...                                        NaN   \n",
       "\n",
       "                                                                                             abstract  \n",
       "title                                                                                                  \n",
       "BERT: Pre-training of Deep Bidirectional Transf...  We introduce a new language representation mod...  \n",
       "OCR-free Document Understanding Transformer         Understanding document images (e.g., invoices)...  \n",
       "BART: Denoising Sequence-to-Sequence Pre-traini...  We present BART, a denoising autoencoder for p...  \n",
       "Attention Is All You Need                           The dominant sequence transduction models are ...  \n",
       "Unifying Vision, Text, and Layout for Universal...  We propose Universal Document Processing (UDOP...  \n",
       "Image-and-Language Understanding from Pixels Only   Multimodal models are becoming increasingly ef...  \n",
       "LayoutLMv2: Multi-modal Pre-training for Visual...  Pre-training of text and layout has proved eff...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bib_parser import parse_bibtex\n",
    "from bib_formatter import save_xlsx\n",
    "\n",
    "# Specify bibtex fields you want to parse:\n",
    "fields = ['title', 'month', 'year', 'author', 'publisher', \n",
    "          'file', 'keywords', 'note', 'abstract']\n",
    "\n",
    "# Get the fields into DataFrame as columns (index=title):\n",
    "bib_df = parse_bibtex(fields, file='bibliography.bib')\n",
    "\n",
    "# Save to formatted .xlsx file:\n",
    "save_xlsx(bib_df, \"overview.xlsx\", cols=fields)\n",
    "\n",
    "# Display the DataFrame:\n",
    "bib_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e8565-252a-4789-9248-e6e5a32f014e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
